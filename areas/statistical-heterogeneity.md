# Statistical Heterogeneity

* Federated Learning with Non-IID Data [[Paper]](https://arxiv.org/abs/1806.00582) 
* Federated Optimization for Heterogeneous Networks [[Paper]](https://arxiv.org/pdf/1812.06127)
* Performance Optimization for Federated Person Re-identification via Benchmark Analysis [[Paper]](https://arxiv.org/abs/2008.11560) [ACMMM20] [[Github]](https://github.com/cap-ntu/FedReID) 
* Characterizing Impacts of Heterogeneity in Federated Learning upon Large-Scale Smartphone Data [[Paper]](https://arxiv.org/abs/2006.06983)

## Distributed Optimization

* Asynchronous Federated Optimization [[Paper]](https://arxiv.org/abs/1903.03934)
* Agnostic Federated Learning [[Paper]](https://arxiv.org/abs/1902.00146) (ICML 2019)
* High Dimensional Restrictive Federated Model Selection with multi-objective Bayesian Optimization over shifted distributions [[Paper]](https://arxiv.org/pdf/1902.08999)
* FedSplit: an algorithmic framework for fast federated optimization [[Paper]](https://papers.nips.cc/paper/2020/file/4ebd440d99504722d80de606ea8507da-Paper.pdf) [Berkeley] [NIPS20]
* Federated Accelerated Stochastic Gradient Descent [[Paper]](https://papers.nips.cc/paper/2020/file/39d0a8908fbe6c18039ea8227f827023-Paper.pdf) [[Github]](https://github.com/hongliny/FedAc-NeurIPS20) [Stanford] [NIPS20]
* Distributionally Robust Federated Averaging [[Paper]](https://papers.nips.cc/paper/2020/file/ac450d10e166657ec8f93a1b65ca1b14-Paper.pdf)
* Tackling the Objective Inconsistency Problem in Heterogeneous Federated Optimization [[Paper]](https://papers.nips.cc/paper/2020/file/564127c03caab942e503ee6f810f54fd-Paper.pdf) [CMU] [NIPS20]
* Federated Bayesian Optimization via Thompson Sampling [[Paper]](https://papers.nips.cc/paper/2020/file/6dfe08eda761bd321f8a9b239f6f4ec3-Paper.pdf) [NUS] [MIT] [NIPS20]


## Model Aggregation

* Federated Learning with Matched Averaging [[Paper]](https://openreview.net/forum?id=BkluqlSFDS) (ICLR 2020)
* Minibatch vs Local SGD for Heterogeneous Distributed Learning [[Paper]](https://papers.nips.cc/paper/2020/file/45713f6ff2041d3fdfae927b82488db8-Paper.pdf) [Toyota] [NIPS20]
* An Efficient Framework for Clustered Federated Learning [[Paper]](https://papers.nips.cc/paper/2020/file/e32cc80bf07915058ce90722ee17bb71-Paper.pdf) [Berkeley] [NIPS20]
* Robust Federated Learning: The Case of Affine Distribution Shifts [[Paper]](https://papers.nips.cc/paper/2020/file/f5e536083a438cec5b64a4954abc17f1-Paper.pdf) [MIT] [NIPS20]
* Bayesian Nonparametric Federated Learning of Neural Networks [[Paper]](https://arxiv.org/abs/1905.12022) (ICML 2019)
* Asynchronous Federated Learning for Geospatial Applications [[Paper]](https://link.springer.com.remotexs.ntu.edu.sg/chapter/10.1007/978-3-030-14880-5_2) [ECML PKDD Workshop 2018] 
* Adaptive Federated Learning in Resource Constrained Edge Computing Systems [[Paper]](https://arxiv.org/abs/1804.05271) [IEEE Journal on Selected Areas in Communications, 2019]
* Towards Faster and Better Federated Learning: A Feature Fusion Approach [[Paper]](https://ieeexplore.ieee.org/abstract/document/8803001/) [ICIP 2019]
* Split Learning: Distributed and collaborative learning [[Paper]](https://aiforsocialgood.github.io/iclr2019/accepted/track1/pdfs/31_aisg_iclr2019.pdf)
* Multi-objective Evolutionary Federated Learning [[Paper]](https://arxiv.org/pdf/1812.07478)

### Knowledge Distillation

* Distributed Distillation for On-Device Learning [[Paper]](https://papers.nips.cc/paper/2020/file/fef6f971605336724b5e6c0c12dc2534-Paper.pdf) [Stanford]
* **Ensemble Distillation for Robust Model Fusion in Federated Learning** [[Paper]](https://papers.nips.cc/paper/2020/file/18df51b97ccd68128e994804f3eccc87-Paper.pdf) 
* Group Knowledge Transfer: Federated Learning of Large CNNs at the Edge [[Paper]](https://papers.nips.cc/paper/2020/file/a1d4c20b182ad7137ab3606f0e3fc8a4-Paper.pdf) [USC]
* One-Shot Federated Learning [[Paper]](https://arxiv.org/abs/1902.11175)


## Personalization & Meta Learning

* Personalized Federated Learning with Moreau Envelopes [[Paper]](https://arxiv.org/abs/2006.08848) [NIPS20]
* Lower Bounds and Optimal Algorithms for Personalized Federated Learning [[Paper]](https://papers.nips.cc/paper/2020/file/187acf7982f3c169b3075132380986e4-Paper.pdf) [KAUST] [NIPS20]
* Personalized Federated Learning with Theoretical Guarantees: A Model-Agnostic Meta-Learning Approach [[Paper]](https://papers.nips.cc/paper/2020/file/24389bfe4fe2eba8bf9aa9203a44cdad-Paper.pdf) [MIT] [NIPS20]
* Improving Federated Learning Personalization via Model Agnostic Meta Learning [[Paper]](https://arxiv.org/abs/1909.12488) [NIPS 2019]Workshop)
* Federated Meta-Learning with Fast Convergence and Efficient Communication [[Paper]](https://arxiv.org/abs/1802.07876)
* Federated Meta-Learning for Recommendation [[Paper]](https://www.semanticscholar.org/paper/Federated-Meta-Learning-for-Recommendation-Chen-Dong/8e21d353ba283bee8fd18285558e5e8df39d46e8#paper-header)
* Adaptive Gradient-Based Meta-Learning Methods [[Paper]](https://arxiv.org/abs/1906.02717)
* Local Learning Matters: Rethinking Data Heterogeneity in Federated Learning [[Paper]](https://ieeexplore.ieee.org/document/9880310)

## Multi-task Learning

* MOCHA: Federated Multi-Task Learning [[Paper]](https://arxiv.org/abs/1705.10467) [[NIPS 2017]](https://papers.nips.cc/paper/7029-federated-multi-task-learning) [[Slides]](http://learningsys.org/nips17/assets/slides/mocha-NIPS.pdf)
* Variational Federated Multi-Task Learning [[Paper]](https://arxiv.org/abs/1906.06268)
* Federated Kernelized Multi-Task Learning [[Paper]](https://mlsys.org/Conferences/2019/doc/2018/30.pdf)
* Clustered Federated Learning: Model-Agnostic Distributed Multi-Task Optimization under Privacy Constraints [[Paper]](https://arxiv.org/abs/1910.01991) [NIPS 2019 Workshop]

## Others

* Distributed Training with Heterogeneous Data: Bridging Median- and Mean-Based Algorithms [[Paper]](https://papers.nips.cc/paper/2020/file/f629ed9325990b10543ab5946c1362fb-Paper.pdf) [NIPS20]
* Distributed Fine-tuning of Language Models on Private Data [[Paper]](https://openreview.net/pdf?id=HkgNdt26Z) [ICLR 2018]
* Federated Learning with Unbiased Gradient Aggregation and Controllable Meta Updating [[Paper]](https://arxiv.org/abs/1910.08234) [NIPS 2019 Workshop]
* The Non-IID Data Quagmire of Decentralized Machine Learning [[Paper]](https://arxiv.org/abs/1910.00189)
* Robust and Communication-Efficient Federated Learning from Non-IID Data [[Paper]](https://arxiv.org/pdf/1903.02891) [IEEE transactions on neural networks and learning systems]
* FedMD: Heterogenous Federated Learning via Model Distillation [[Paper]](https://arxiv.org/abs/1910.03581) [NIPS 2019 Workshop]
* First Analysis of Local GD on Heterogeneous Data [[Paper]](https://arxiv.org/abs/1909.04715)
* SCAFFOLD: Stochastic Controlled Averaging for On-Device Federated Learning [[Paper]](https://arxiv.org/abs/1910.06378) - ICML20
* On the Convergence of FedAvg on Non-IID Data [[Paper]](https://arxiv.org/abs/1907.02189) [[OpenReview]](https://openreview.net/forum?id=HJxNAnVtDS)
* Agnostic Federated Learning [[Paper]](https://arxiv.org/abs/1902.00146) (ICML 2019)
* Local SGD Converges Fast and Communicates Little [[Paper]](https://arxiv.org/abs/1805.09767)
* Federated Adversarial Domain Adaptation [[Paper]](https://arxiv.org/abs/1911.02054) (ICLR 2020)
* LoAdaBoost: Loss-Based AdaBoost Federated Machine Learning on Medical Data [[Paper]](https://arxiv.org/pdf/1811.12629)
* On Federated Learning of Deep Networks from Non-IID Data: Parameter Divergence and the Effects of Hyperparametric Methods [[Paper]](https://openreview.net/forum?id=SJeOAJStwB) [Rejected in ICML 2020]
* Overcoming Forgetting in Federated Learning on Non-IID Data [[Paper]](https://arxiv.org/abs/1910.07796) [NIPS 2019 Workshop]
* FedMAX: Activation Entropy Maximization Targeting Effective Non-IID Federated Learning [[Video]](#workshop) [NIPS 2019 Workshop]
* Measuring the Effects of Non-Identical Data Distribution for Federated Visual Classification [[Paper]](https://arxiv.org/abs/1909.06335) [NIPS 2019 Workshop]
* Fair Resource Allocation in Federated Learning [[Paper]](https://arxiv.org/abs/1905.10497)
* Communication-efficient on-device machine learning: Federated distillation and augmentation under non-iid private data [[Paper]](https://arxiv.org/abs/1811.11479)
* Think Locally, Act Globally: Federated Learning with Local and Global Representations [[Paper]](https://arxiv.org/abs/2001.01523) [NIPS 2019 Workshop]
* A Linear Speedup Analysis of Distributed Deep Learning with Sparse and Quantized Communication [[Paper]](https://papers.nips.cc/paper/7519-a-linear-speedup-analysis-of-distributed-deep-learning-with-sparse-and-quantized-communication) [NIPS 2018]
* SCAFFOLD: Stochastic Controlled Averaging for On-Device Federated Learning [[Paper]](https://arxiv.org/abs/1910.06378)
* On the Convergence of FedAvg on Non-IID Data [[Paper]](https://arxiv.org/abs/1907.02189) [[OpenReview]](https://openreview.net/forum?id=HJxNAnVtDS)
* Can Decentralized Algorithms Outperform Centralized Algorithms? A Case Study for Decentralized Parallel Stochastic Gradient Descent [[Paper]](https://arxiv.org/abs/1705.09056) [NIPS 2017]
* Communication Efficient Decentralized Training with Multiple Local Updates [[Paper]](https://arxiv.org/abs/1910.09126)
* First Analysis of Local GD on Heterogeneous Data [[Paper]](https://arxiv.org/abs/1909.04715)
* MATCHA: Speeding Up Decentralized SGD via Matching Decomposition Sampling [[Paper]](https://arxiv.org/abs/1905.09435)
* Local SGD Converges Fast and Communicates Little [[Paper]](https://arxiv.org/abs/1805.09767)
* SlowMo: Improving Communication-Efficient Distributed SGD with Slow Momentum [[Paper]](https://arxiv.org/abs/1910.00643)
* Adaptive Federated Learning in Resource Constrained Edge Computing Systems [[Paper]](https://arxiv.org/abs/1804.05271) [IEEE Journal on Selected Areas in Communications, 2019]
* Parallel Restarted SGD with Faster Convergence and Less Communication: Demystifying Why Model Averaging Works for Deep Learning [[Paper]](https://arxiv.org/abs/1807.06629) [AAAI 2018]
* On the Linear Speedup Analysis of Communication Efficient Momentum SGD for Distributed Non-Convex Optimization [[Paper]](https://arxiv.org/abs/1905.03817) [ICML 2019]
*  Communication-efficient on-device machine learning: Federated distillation and augmentation under non-iid private data [[Paper]](https://arxiv.org/abs/1811.11479)
* Convergence of Distributed Stochastic Variance Reduced Methods without Sampling Extra Data [[Paper]](https://arxiv.org/abs/1905.12648) [NIPS 2019 Workshop]
